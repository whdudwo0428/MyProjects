import PyPDF2
import nltk
import re
import sys
import numpy as np
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from gensim import corpora
from gensim.models import LdaModel
from gensim.models.coherencemodel import CoherenceModel



## 0. PDF ì½ê¸°
def read_pdf(file_path):
    with open(file_path, "rb") as f:
        return "".join([p.extract_text() + "\n" for p in PyPDF2.PdfReader(f).pages])


## 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (í•œ ë²ˆë§Œ ì‹¤í–‰)
def preprocess_text(text):
    text = text.lower()
    text = ''.join([char if char.isalnum() or char.isspace() else ' ' for char in text])
    tokens = word_tokenize(text)

    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if
              token not in stop_words and not re.fullmatch(r'\d+', token) and len(token) > 1]

    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    return ' '.join(tokens)


## 2. BoW, TF-IDF ë³€í™˜ í•¨ìˆ˜ (í•œ ë²ˆë§Œ ì‹¤í–‰)
def extract_features(text):
    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform([text])

    word_list = vectorizer.get_feature_names_out()
    word_counts = np.asarray(X.sum(axis=0)).flatten()
    word_freq = sorted(zip(word_list, word_counts), key=lambda x: x[1], reverse=True)

    tfidf_vectorizer = TfidfVectorizer()
    X_tfidf = tfidf_vectorizer.fit_transform([text])
    tfidf_word_list = tfidf_vectorizer.get_feature_names_out()
    tfidf_scores = np.asarray(X_tfidf.sum(axis=0)).flatten()
    tfidf_freq = sorted(zip(tfidf_word_list, tfidf_scores), key=lambda x: x[1], reverse=True)

    return word_freq, tfidf_freq


## 3. LDA ìµœì  í† í”½ ê°œìˆ˜ ì°¾ê¸°
def find_optimal_topics(dictionary, corpus, texts, start=2, limit=10, step=1):
    scores = []
    best_num_topics = start
    best_coherence = 0

    print("\nðŸ”„ [LDA ìµœì  í† í”½ ê°œìˆ˜ ì°¾ê¸° ì§„í–‰ ì¤‘...]")
    for num in range(start, limit, step):
        sys.stdout.write(f"\râ–¶ Checking num_topics = {num}... \n")
        sys.stdout.flush()

        lda_model = LdaModel(corpus, num_topics=num, id2word=dictionary, passes=30, iterations=150)
        coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_score = coherence_model_lda.get_coherence()
        scores.append((num, coherence_score))

        if coherence_score > best_coherence:
            best_coherence = coherence_score
            best_num_topics = num

    print("\nâœ… ìµœì ì˜ í† í”½ ê°œìˆ˜ ê²°ì • ì™„ë£Œ.")
    return best_num_topics


# âœ… ì‹¤í–‰ ì½”ë“œ (ì¶œë ¥ í•œ ë²ˆë§Œ!)
if __name__ == "__main__":
    text = read_pdf("Data/Paper_PDF2_PointRCNNpdf.pdf")         # pdf íŒŒì¼ ì„ íƒ

    # ì „ì²˜ë¦¬ (í•œ ë²ˆë§Œ ì‹¤í–‰)
    processed_text = preprocess_text(text)

    print("\nðŸ“Œ [í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê²°ê³¼]")
    print(f"ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ì¼ë¶€ (ì•ž 1000ìž):\n{processed_text[:1000]}")

    # BoW, TF-IDF (í•œ ë²ˆë§Œ ì‹¤í–‰)
    word_freq, tfidf_freq = extract_features(processed_text)

    print("\nðŸ“Œ [BoW ë³€í™˜ ê²°ê³¼]")
    print(f"ì´ ë‹¨ì–´ ê°œìˆ˜ (Vocabulary Size): {len(word_freq)}")
    print(f"ì¼ë¶€ ë‹¨ì–´ ëª©ë¡ (Top 20): {', '.join([word for word, _ in word_freq[:20]])}")

    print(f"\nðŸ“Œ [ë¹ˆë„ ë†’ì€ ë‹¨ì–´ Top 20]")
    for word, freq in word_freq[:20]:
        print(f"{word}: {freq}")

    print(f"\nðŸ“Œ [TF-IDF ê°€ì¤‘ì¹˜ Top 20]")
    for word, score in tfidf_freq[:20]:
        print(f"{word}: {score:.4f}")

    # âœ… LDA í† í”½ ëª¨ë¸ë§ (ì—¬ê¸°ì„œë¶€í„° ë°˜ë³µ ì¶œë ¥ ì•ˆ ë¨)
    tokenized_text = processed_text.split()
    dictionary = corpora.Dictionary([tokenized_text])
    corpus = [dictionary.doc2bow(tokenized_text)]

    optimal_num_topics = find_optimal_topics(dictionary, corpus, [tokenized_text], start=2, limit=10)
    print(f"âœ… ìµœì ì˜ í† í”½ ê°œìˆ˜: {optimal_num_topics}")

    # âœ… **LDA ëª¨ë¸ ìµœì ì˜ num_topicsë¡œ í•™ìŠµ**
    print("\nðŸ”„ [LDA ëª¨ë¸ ìµœì¢… í•™ìŠµ ì§„í–‰...]")
    lda_model = LdaModel(corpus, num_topics=optimal_num_topics,
                         id2word=dictionary, passes=50, iterations=300,
                         alpha='auto', eta='auto', random_state=42)

    # âœ… **ìµœì¢… LDA ê²°ê³¼ ì¶œë ¥**
    print(f"\nðŸ“Œ [LDA í† í”½ ëª¨ë¸ë§ ê²°ê³¼ (Topic number = {optimal_num_topics})]")
    for idx, topic in lda_model.show_topics(num_topics=optimal_num_topics, num_words=10, formatted=False):
        words_with_probs = " + ".join([f"{prob:.3f}*\"{word}\"" for word, prob in topic])  # í™•ë¥  í¬í•¨
        words_only = ", ".join([word for word, _ in topic])  # í™•ë¥  ì œê±°í•œ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸

        print(f"- í† í”½ {idx + 1}: {words_with_probs}")
        print(f"   => {words_only}\n")
